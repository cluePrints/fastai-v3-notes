{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fastai3_part2: 02_fully_connected.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cluePrints/fastai-v3-notes/blob/master/fastai3_part2_02_fully_connected.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "T1h-Y-nShkLM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iuGzOMwKiEOJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1.0.50 is the earliest version course materials supposed to be working with\n",
        "pip install fastai>=1.0.50 -q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FWY-6KHCiCez",
        "colab_type": "code",
        "outputId": "32ccb8f5-ca00-463c-ce5f-e5d79be74014",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip freeze | grep fastai"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fastai==1.0.50.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vDv2B-Juhs8a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# comes from prev lesson\n",
        "import operator\n",
        "\n",
        "def test(a,b,cmp,cname=None):\n",
        "    if cname is None: cname=cmp.__name__\n",
        "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
        "\n",
        "def test_eq(a,b): test(a,b,operator.eq,'==')\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.core.debugger import set_trace\n",
        "from fastai import datasets\n",
        "import pickle, gzip, math, torch, matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import tensor\n",
        "\n",
        "MNIST_URL='http://deeplearning.net/data/mnist/mnist.pkl'\n",
        "\n",
        "def near(a,b): return torch.allclose(a, b, rtol=1e-3, atol=1e-5)\n",
        "def test_near(a,b): test(a,b,near)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-rcJQjfoiA1w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from fastai import datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VHAWBG_OQumA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = datasets.download_data(MNIST_URL, ext='.gz')\n",
        "with gzip.open(path) as f:\n",
        "  ((train_X, train_Y), (valid_X, valid_Y), (test_X, test_Y)) = pickle.load(f, encoding='latin')\n",
        "\n",
        "# was running out of memory too much\n",
        "train_X = torch.tensor(train_X)[:1000]\n",
        "train_Y = torch.tensor(train_Y)[:1000]\n",
        "valid_X = torch.tensor(valid_X)\n",
        "valid_Y = torch.tensor(valid_Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kfAmz4FNRj6C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_X = train_X[0][None]\n",
        "batch_Y = train_Y[0][None]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jSHSz5kNaJ6T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c1a9252-e938-46c9-9851-5c4bcc1692eb"
      },
      "cell_type": "code",
      "source": [
        "train_mean, train_std = train_X.mean(), train_X.std()\n",
        "train_mean,train_std"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1277), tensor(0.3039))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "DweMYgms03cq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize(x, mean, std):\n",
        "  return (x-mean) / std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sd3nZkv8thBm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_X = normalize(train_X, train_mean, train_std)\n",
        "valid_X = normalize(valid_X, train_mean, train_std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o4_R4xHQtvoT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe3db905-4be4-4058-cfb1-0750d4dc5fd7"
      },
      "cell_type": "code",
      "source": [
        "train_X.mean(), train_X.std()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1.0950e-06), tensor(1.))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "oFq_1x191BZW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f156926d-a258-40e6-9c5d-9afc21d38795"
      },
      "cell_type": "code",
      "source": [
        "valid_X.mean(), valid_X.std()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.0030), tensor(1.0033))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "bfeNcbke-yHS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34029334-b5ed-4260-b9c4-9667c9a6cd57"
      },
      "cell_type": "code",
      "source": [
        "train_X.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 784])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "xA79s-eZ1G7v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_inputs = train_X.shape[-1]\n",
        "n_hidden = 100\n",
        "n_outputs = 1\n",
        "w1 = torch.zeros(n_inputs, n_hidden)\n",
        "b1 = torch.zeros(n_hidden)\n",
        "w2 = torch.zeros(n_hidden, n_outputs)\n",
        "b2 = torch.zeros(n_hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sl8Q-hoP_Ijl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "f7fea0ab-308b-4b45-9025-31a15dc3ba49"
      },
      "cell_type": "code",
      "source": [
        "(torch.tensor(train_X) @ w1).shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "XdAvn49C_nJS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3c7e1ca-07ff-4dca-84c9-4644578b054c"
      },
      "cell_type": "code",
      "source": [
        "class Lin():\n",
        "  def __init__(self, n_inputs, n_outputs):\n",
        "    self.n_inputs, self.n_outputs = n_inputs, n_outputs\n",
        "    self.w = torch.randn(n_inputs, n_outputs)*math.sqrt(2/n_inputs)\n",
        "    self.b = torch.zeros(n_outputs)\n",
        "    \n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return x @ self.w + self.b\n",
        "\n",
        "class ReLU():\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.clamp_min(x, 0)\n",
        "\n",
        "\n",
        "# forward\n",
        "class Model:\n",
        "  def __init__(self):\n",
        "    self.lin1 = Lin(n_inputs, n_hidden)\n",
        "    self.lin2 = Lin(n_hidden, n_outputs)\n",
        "    self.relu = ReLU()\n",
        "    \n",
        "  def __call__(self, x):\n",
        "    x = self.lin1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.lin2(x)\n",
        "    return x\n",
        "\n",
        "model = Model()\n",
        "x = model(train_X)\n",
        "  \n",
        "x.mean(), x.std()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.6355), tensor(0.9962))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "amzIv-bBHaIv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bbf67099-7639-406a-967f-0d54a53e79f8"
      },
      "cell_type": "code",
      "source": [
        "x.shape, train_Y.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1000, 1]), torch.Size([1000]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "0sQSwFgaI5cA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mse(actual, expected):\n",
        "  return (expected - actual).pow(2).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f_A5dLV6H53h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "00a243d7-3b75-4681-f29f-9bc4016fe534"
      },
      "cell_type": "code",
      "source": [
        "x.shape, train_Y.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1000, 1]), torch.Size([1000]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "RML3opNvF9Id",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2228ced9-665f-4206-fa3d-8fcbbde87927"
      },
      "cell_type": "code",
      "source": [
        "mse(x, train_Y[:,None].float())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(24.4955)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "EPD-KffbICwT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pdb\n",
        "def mse_grad(activations, target):\n",
        "  # f(g(x)) = f'(g(x))*g'(x)\n",
        "  # d/dx(mse(lin2(...), y)) = d/dx((lin(x) - y)^2/count) = 2(lin(x)-y)/count * d/dx(lin2(...))\n",
        "  grad = 2 * (activations - target) / len(activations)\n",
        "  assert grad.shape == activations.shape\n",
        "  return grad\n",
        "\n",
        "def lin_grad(prev_g, layer, layer_input):\n",
        "  # lin(u=input, w, b) * prev_g = (d/du(u*w+b) + d/dw(u*w+b) + d/db(u*w+b))*prev_g = (w + u + 1)*prev_g\n",
        "  inp_grad = prev_g @ layer.w.t()\n",
        "  w_grad = layer_input.t() @ prev_g\n",
        "  # Note to self: original way of calculating the gradient was OOMing\n",
        "  # orig_w_grad = (layer_input.unsqueeze(-1) * prev_g.unsqueeze(1)).sum(0)\n",
        "  b_grad = prev_g.sum(0)\n",
        "  assert inp_grad.shape == layer_input.shape\n",
        "  assert w_grad.shape == layer.w.shape\n",
        "  assert b_grad.shape == layer.b.shape\n",
        "  return (inp_grad, w_grad, b_grad)\n",
        "\n",
        "def relu_grad(lin2_grad_part, relu_ins):\n",
        "  grad = lin2_grad_part * (relu_ins > 0).float()\n",
        "  assert grad.shape == relu_ins.shape\n",
        "  return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1BjWgig3KcvL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab8c2f40-304b-4595-b51b-c30ba1325f8e"
      },
      "cell_type": "code",
      "source": [
        "# forward\n",
        "lin1 = Lin(n_inputs, n_hidden)\n",
        "lin2 = Lin(n_hidden, n_outputs)\n",
        "relu = ReLU()\n",
        "lin1_out = lin1(train_X)\n",
        "relu_out = relu(lin1_out)\n",
        "lin2_out = lin2(relu_out)\n",
        "\n",
        "# ...and backward\n",
        "loss = mse(lin2_out, train_Y[:,None].float())\n",
        "\n",
        "grad = mse_grad(lin2_out, train_Y[:,None].float())\n",
        "grad, w2_grad, b2_grad = lin_grad(grad, lin2, relu_out)\n",
        "grad = relu_grad(grad, lin1_out)\n",
        "grad, w1_grad, b1_grad = lin_grad(grad, lin1, train_X)\n",
        "grad.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 784])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "gvViB-f7uPUf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Lin():\n",
        "  def __init__(self, n_inputs, n_outputs, requires_grad=True):\n",
        "    self.n_inputs, self.n_outputs = n_inputs, n_outputs\n",
        "    self.w = torch.randn(n_inputs, n_outputs, requires_grad=requires_grad)*math.sqrt(2/n_inputs)\n",
        "    # Note to self: this generated no grads initially because I fed X and not lin2_out to the mse, so 'w' was no a leaf\n",
        "    # necessity to use retain_grad() is just a symtom of me screwing up the chain of calcs\n",
        "    # self.w.retain_grad()\n",
        "    self.b = torch.zeros(n_outputs, requires_grad=requires_grad)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return x @ self.w + self.b\n",
        "\n",
        "class ReLU():\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.clamp_min(x, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kB0-hVFztjKf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def verify_grads(lin1_param, lin2_param, w1_grad, b1_grad, w2_grad, b2_grad):\n",
        "  train_X_w_grad = train_X.clone().requires_grad_(True)\n",
        "  l1 = Lin(n_inputs, n_hidden)\n",
        "  l1.w = lin1_param.w.clone().requires_grad_(True)\n",
        "  l1.b = lin1_param.b.clone().requires_grad_(True)\n",
        "  l2 = Lin(n_hidden, n_outputs)\n",
        "  l2.w = lin2_param.w.clone().requires_grad_(True)\n",
        "  l2.b = lin2_param.b.clone().requires_grad_(True)\n",
        "\n",
        "  relu = ReLU()\n",
        "  l1_out = l1(train_X_w_grad)\n",
        "  r_out = relu(l1_out)\n",
        "  l2_out = l2(r_out)\n",
        "  tmp_loss = mse(l2_out, train_Y[:,None].float())\n",
        "  tmp_loss.backward()\n",
        "\n",
        "  test_near(l1_out, lin1_out)\n",
        "  test_near(l2_out, lin2_out)\n",
        "  test_near(r_out,  relu_out)\n",
        "  test_near(tmp_loss,  loss)\n",
        "  test_near(b1_grad, l1.b.grad)\n",
        "  test_near(b2_grad, l2.b.grad)\n",
        "  test_near(w1_grad, l1.w.grad)\n",
        "  test_near(w2_grad, l2.w.grad)\n",
        "\n",
        "verify_grads(lin1, lin2, w1_grad, b1_grad, w2_grad, b2_grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WapIxH0T70LM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Refactor to a model\n"
      ]
    },
    {
      "metadata": {
        "id": "-BPA9ecjFP2E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Lin():\n",
        "  def __init__(self, n_inputs, n_outputs):\n",
        "    self.n_inputs, self.n_outputs = n_inputs, n_outputs\n",
        "    self.w = torch.randn(n_inputs, n_outputs)*math.sqrt(2/n_inputs)\n",
        "    self.b = torch.zeros(n_outputs)\n",
        "    \n",
        "  def __call__(self, x):\n",
        "    self.input = x\n",
        "    self.output = self.forward(x)\n",
        "    return self.output\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return x @ self.w + self.b\n",
        "\n",
        "class ReLU():\n",
        "  def __call__(self, x):\n",
        "    self.input = x\n",
        "    self.output = self.forward(x)\n",
        "    return self.output\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.clamp_min(x, 0)\n",
        "\n",
        "\n",
        "class Model:\n",
        "  def __init__(self):\n",
        "    self.lin1 = Lin(n_inputs, n_hidden)\n",
        "    self.lin2 = Lin(n_hidden, n_outputs)\n",
        "    self.relu = ReLU()\n",
        "    \n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.input = x\n",
        "    x = self.lin1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.lin2(x)\n",
        "    return x\n",
        "\n",
        "  def backward(self, y):\n",
        "    self.loss = mse(self.lin2.output, y)\n",
        "    grad_mse = mse_grad(self.lin2.output, y)\n",
        "    self.grad_lin2, self.grad_w2, self.grad_b2 = lin_grad(grad_mse, self.lin2, self.relu.output)\n",
        "    grad_relu = relu_grad(self.grad_lin2, self.lin1.output)\n",
        "    self.grad_lin1, self.grad_w1, self.grad_b1 = lin_grad(grad_relu, self.lin1, self.lin1.input)\n",
        "\n",
        "  def verify_grads(self):\n",
        "    train_X_w_grad = self.input.clone().requires_grad_(True)\n",
        "    l1 = Lin(n_inputs, n_hidden)\n",
        "    l1.w = self.lin1.w.clone().requires_grad_(True)\n",
        "    l1.b = self.lin1.b.clone().requires_grad_(True)\n",
        "    l2 = Lin(n_hidden, n_outputs)\n",
        "    l2.w = self.lin2.w.clone().requires_grad_(True)\n",
        "    l2.b = self.lin2.b.clone().requires_grad_(True)\n",
        "\n",
        "    relu = ReLU()\n",
        "    l1_out = l1(train_X_w_grad)\n",
        "    r_out = relu(l1_out)\n",
        "    l2_out = l2(r_out)\n",
        "    tmp_loss = mse(l2_out, train_Y[:,None].float())\n",
        "    tmp_loss.backward()\n",
        "\n",
        "    test_near(self.grad_b1, l1.b.grad)\n",
        "    test_near(self.grad_b2, l2.b.grad)\n",
        "    test_near(self.grad_w1, l1.w.grad)\n",
        "    test_near(self.grad_w2, l2.w.grad)\n",
        "    \n",
        "model = Model()\n",
        "model.forward(train_X)\n",
        "model.backward(train_Y[:,None].float())\n",
        "model.verify_grads()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-RqAgQKuFdkF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# self.target added\n",
        "# Module refactored out common stuff\n",
        "# backward() extracted on lin2\n",
        "\n",
        "class Module():\n",
        "  def __call__(self, x):\n",
        "    self.input = x\n",
        "    self.output = self.forward(x)\n",
        "    return self.output\n",
        "\n",
        "class Lin(Module):\n",
        "  def __init__(self, n_inputs, n_outputs):\n",
        "    self.n_inputs, self.n_outputs = n_inputs, n_outputs\n",
        "    self.w = torch.randn(n_inputs, n_outputs)*math.sqrt(2/n_inputs)\n",
        "    self.b = torch.zeros(n_outputs)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return x @ self.w + self.b\n",
        "  \n",
        "  def backward(self):\n",
        "    grad, grad_w2, grad_b2 = lin_grad(self.output.g, self, self.input)\n",
        "    self.g = grad\n",
        "    self.w.g = grad_w2\n",
        "    self.b.g = grad_b2\n",
        "\n",
        "class ReLU(Module):\n",
        "  def forward(self, x):\n",
        "    return torch.clamp_min(x, 0)\n",
        "  \n",
        "\n",
        "\n",
        "class Model:\n",
        "  def __init__(self):\n",
        "    self.lin1 = Lin(n_inputs, n_hidden)\n",
        "    self.lin2 = Lin(n_hidden, n_outputs)\n",
        "    self.relu = ReLU()\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    self.input = x\n",
        "    self.target = y\n",
        "    x = self.lin1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.lin2(x)\n",
        "    return x\n",
        "\n",
        "  def backward(self):\n",
        "    self.loss = mse(self.lin2.output, self.target)\n",
        "    grad_mse = mse_grad(self.lin2.output, self.target)\n",
        "    self.lin2.output.g = grad_mse\n",
        "    self.lin2.backward()\n",
        "    grad_relu = relu_grad(self.lin2.g, self.lin1.output)\n",
        "    self.grad_lin1, self.grad_w1, self.grad_b1 = lin_grad(grad_relu, self.lin1, self.lin1.input)\n",
        "\n",
        "  def verify_grads(self):\n",
        "    train_X_w_grad = self.input.clone().requires_grad_(True)\n",
        "    l1 = Lin(n_inputs, n_hidden)\n",
        "    l1.w = self.lin1.w.clone().requires_grad_(True)\n",
        "    l1.b = self.lin1.b.clone().requires_grad_(True)\n",
        "    l2 = Lin(n_hidden, n_outputs)\n",
        "    l2.w = self.lin2.w.clone().requires_grad_(True)\n",
        "    l2.b = self.lin2.b.clone().requires_grad_(True)\n",
        "\n",
        "    relu = ReLU()\n",
        "    l1_out = l1(train_X_w_grad)\n",
        "    r_out = relu(l1_out)\n",
        "    l2_out = l2(r_out)\n",
        "    tmp_loss = mse(l2_out, train_Y[:,None].float())\n",
        "    tmp_loss.backward()\n",
        "\n",
        "    test_near(self.grad_b1, l1.b.grad)\n",
        "    test_near(self.lin2.b.g, l2.b.grad)\n",
        "    test_near(self.grad_w1, l1.w.grad)\n",
        "    test_near(self.lin2.w.g, l2.w.grad)\n",
        "    \n",
        "model = Model()\n",
        "model.forward(train_X, train_Y[:,None].float())\n",
        "model.backward()\n",
        "model.verify_grads()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zxRvkrUtHOL8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# backward() extracted on lin1\n",
        "# backward() extracted on relu\n",
        "# backward(), forward() extracted on mse\n",
        "\n",
        "class Module():\n",
        "  def __call__(self, x):\n",
        "    self.input = x\n",
        "    self.output = self.forward(x)\n",
        "    return self.output\n",
        "\n",
        "class Lin(Module):\n",
        "  def __init__(self, n_inputs, n_outputs):\n",
        "    self.n_inputs, self.n_outputs = n_inputs, n_outputs\n",
        "    self.w = torch.randn(n_inputs, n_outputs)*math.sqrt(2/n_inputs)\n",
        "    self.b = torch.zeros(n_outputs)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return x @ self.w + self.b\n",
        "  \n",
        "  def backward(self):\n",
        "    grad, grad_w2, grad_b2 = lin_grad(self.output.g, self, self.input)\n",
        "    self.input.g = grad\n",
        "    self.w.g = grad_w2\n",
        "    self.b.g = grad_b2\n",
        "\n",
        "class ReLU(Module):\n",
        "  def forward(self, x):\n",
        "    return torch.clamp_min(x, 0)\n",
        "\n",
        "  def backward(self):\n",
        "    self.input.g = relu_grad(self.output.g, self.input)\n",
        "\n",
        "class Mse(Module):\n",
        "  def forward(self, x):\n",
        "    self.loss_value = mse(x, self.target)\n",
        "    return x\n",
        "  \n",
        "  def backward(self):\n",
        "    self.input.g = mse_grad(self.input, self.target)\n",
        "\n",
        "class Model:\n",
        "  def __init__(self):\n",
        "    self.lin1 = Lin(n_inputs, n_hidden)\n",
        "    self.lin2 = Lin(n_hidden, n_outputs)\n",
        "    self.relu = ReLU()\n",
        "    self.mse = Mse()\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    self.input = x\n",
        "    self.target = y\n",
        "    self.mse.target = y\n",
        "    x = self.lin1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.lin2(x)\n",
        "    x = self.mse(x)\n",
        "    return x\n",
        "\n",
        "  def backward(self):\n",
        "    self.mse.backward()\n",
        "    self.lin2.backward()\n",
        "    self.relu.backward()\n",
        "    self.lin1.backward()\n",
        "    \n",
        "  def verify_grads(self):\n",
        "    train_X_w_grad = self.input.clone().requires_grad_(True)\n",
        "    l1 = Lin(n_inputs, n_hidden)\n",
        "    l1.w = self.lin1.w.clone().requires_grad_(True)\n",
        "    l1.b = self.lin1.b.clone().requires_grad_(True)\n",
        "    l2 = Lin(n_hidden, n_outputs)\n",
        "    l2.w = self.lin2.w.clone().requires_grad_(True)\n",
        "    l2.b = self.lin2.b.clone().requires_grad_(True)\n",
        "\n",
        "    relu = ReLU()\n",
        "    l1_out = l1(train_X_w_grad)\n",
        "    r_out = relu(l1_out)\n",
        "    l2_out = l2(r_out)\n",
        "    tmp_loss = mse(l2_out, train_Y[:,None].float())\n",
        "    tmp_loss.backward()\n",
        "\n",
        "    test_near(self.lin1.b.g, l1.b.grad)\n",
        "    test_near(self.lin2.b.g, l2.b.grad)\n",
        "    test_near(self.lin1.w.g, l1.w.grad)\n",
        "    test_near(self.lin2.w.g, l2.w.grad)\n",
        "    \n",
        "model = Model()\n",
        "model.forward(train_X, train_Y[:,None].float())\n",
        "model.backward()\n",
        "model.verify_grads()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JJLoS3mDOXnV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# mse -> loss\n",
        "# model.layers introduced\n",
        "class Module():\n",
        "  def __call__(self, x):\n",
        "    self.input = x\n",
        "    self.output = self.forward(x)\n",
        "    return self.output\n",
        "\n",
        "class Lin(Module):\n",
        "  def __init__(self, n_inputs, n_outputs):\n",
        "    self.n_inputs, self.n_outputs = n_inputs, n_outputs\n",
        "    self.w = torch.randn(n_inputs, n_outputs)*math.sqrt(2/n_inputs)\n",
        "    self.b = torch.zeros(n_outputs)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return x @ self.w + self.b\n",
        "  \n",
        "  def backward(self):\n",
        "    grad, grad_w2, grad_b2 = lin_grad(self.output.g, self, self.input)\n",
        "    self.input.g = grad\n",
        "    self.w.g = grad_w2\n",
        "    self.b.g = grad_b2\n",
        "\n",
        "class ReLU(Module):\n",
        "  def forward(self, x):\n",
        "    return torch.clamp_min(x, 0)\n",
        "\n",
        "  def backward(self):\n",
        "    self.input.g = relu_grad(self.output.g, self.input)\n",
        "\n",
        "class Mse(Module):\n",
        "  def forward(self, x):\n",
        "    self.loss_value = mse(x, self.target)\n",
        "    return x\n",
        "  \n",
        "  def backward(self):\n",
        "    self.input.g = mse_grad(self.input, self.target)\n",
        "\n",
        "class Model:\n",
        "  def __init__(self):\n",
        "    self.lin1 = Lin(n_inputs, n_hidden)\n",
        "    self.lin2 = Lin(n_hidden, n_outputs)\n",
        "    self.relu = ReLU()\n",
        "    self.layers = [self.lin1, self.relu, self.lin2]\n",
        "    self.loss = Mse()\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    self.input = x\n",
        "    self.target = y\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    self.loss.target = y\n",
        "    return self.loss(x)\n",
        "\n",
        "  def backward(self):\n",
        "    self.loss.backward()\n",
        "    for layer in reversed(self.layers):\n",
        "      layer.backward()\n",
        "    \n",
        "  def verify_grads(self):\n",
        "    train_X_w_grad = self.input.clone().requires_grad_(True)\n",
        "    l1 = Lin(n_inputs, n_hidden)\n",
        "    l1.w = self.lin1.w.clone().requires_grad_(True)\n",
        "    l1.b = self.lin1.b.clone().requires_grad_(True)\n",
        "    l2 = Lin(n_hidden, n_outputs)\n",
        "    l2.w = self.lin2.w.clone().requires_grad_(True)\n",
        "    l2.b = self.lin2.b.clone().requires_grad_(True)\n",
        "\n",
        "    relu = ReLU()\n",
        "    l1_out = l1(train_X_w_grad)\n",
        "    r_out = relu(l1_out)\n",
        "    l2_out = l2(r_out)\n",
        "    tmp_loss = mse(l2_out, train_Y[:,None].float())\n",
        "    tmp_loss.backward()\n",
        "\n",
        "    test_near(self.lin1.b.g, l1.b.grad)\n",
        "    test_near(self.lin2.b.g, l2.b.grad)\n",
        "    test_near(self.lin1.w.g, l1.w.grad)\n",
        "    test_near(self.lin2.w.g, l2.w.grad)\n",
        "    \n",
        "model = Model()\n",
        "model.forward(train_X, train_Y[:,None].float())\n",
        "model.backward()\n",
        "model.verify_grads()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lUSBd_ZYPvgs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eQSpsA520qX6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* 1) normalize\n",
        "* 2) initialize w & b\n",
        "* 3) relu(lin(x))\n",
        "* 4) model=lin(relu(lin(x))\n",
        "* 5) MSE loss func\n",
        "* 6) gradients for all the layers, autograd check\n",
        "* 7) refactor to model\n"
      ]
    }
  ]
}